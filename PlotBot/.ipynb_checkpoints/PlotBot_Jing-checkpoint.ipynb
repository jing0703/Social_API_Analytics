{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import requests as req\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "# Twitter API Keys\n",
    "from config import (consumer_key, \n",
    "                    consumer_secret, \n",
    "                    access_token, \n",
    "                    access_token_secret)\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_replied_tweet = [None]\n",
    "\n",
    "def SentimentTweet():\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "    \n",
    "    request_user_list = []\n",
    "    search_target_list = []\n",
    "    sentiments = []\n",
    "    \n",
    "    my_tweets = api.mentions_timeline(since_id = last_replied_tweet[0])\n",
    "    last_replied_tweet[0] = my_tweets[0][\"id\"]\n",
    "    for tweet in my_tweets:\n",
    "        #print(json.dumps(tweet, sort_keys=True, indent=4))\n",
    "        try:\n",
    "            if tweet[\"text\"] == \"@\"+tweet[\"entities\"][\"user_mentions\"][1][\"screen_name\"] +\" Analyze @\" + tweet[\"entities\"][\"user_mentions\"][1][\"screen_name\"] and tweet[\"entities\"][\"user_mentions\"][1][\"screen_name\"] not in search_target_list:\n",
    "                search_target_list.append(tweet[\"entities\"][\"user_mentions\"][1][\"screen_name\"])\n",
    "                request_user_list.append(tweet[\"user\"][\"screen_name\"])\n",
    "                \n",
    "            elif tweet[\"text\"] == \"@\"+tweet[\"entities\"][\"user_mentions\"][1][\"screen_name\"] +\" Analyze @\" + tweet[\"entities\"][\"user_mentions\"][1][\"screen_name\"] and tweet[\"entities\"][\"user_mentions\"][1][\"screen_name\"] in search_target_list:\n",
    "                print(f'{tweet[\"entities\"][\"user_mentions\"][1][\"screen_name\"]} has been analyzed,try someone else.')\n",
    "            else:\n",
    "                print(f'sorry {tweet[\"text\"]} is not found')\n",
    "        except:\n",
    "            pass\n",
    "    print(len(my_tweets))\n",
    "    print(search_target_list)\n",
    "    print(request_user_list)\n",
    "\n",
    "    for target in search_target_list:\n",
    "        counter =1\n",
    "        oldest_tweet = None\n",
    "        for x in range(25):\n",
    "            public_tweets = api.user_timeline(target, max_id = oldest_tweet)\n",
    "            for tweet in public_tweets:\n",
    "                results = analyzer.polarity_scores(tweet[\"text\"])\n",
    "                compound = results[\"compound\"]\n",
    "                pos = results[\"pos\"]\n",
    "                neu = results[\"neu\"]\n",
    "                neg = results[\"neg\"]\n",
    "                tweets_ago = counter\n",
    "\n",
    "                oldest_tweet = tweet['id'] - 1\n",
    "\n",
    "                sentiments.append({\"User\": target,\"Date\": tweet[\"created_at\"], \n",
    "                                \"Compound\": compound,\"Positive\": pos,\n",
    "                                \"Negative\": neu,\"Neutral\": neg,\n",
    "                                \"Tweets Ago\": counter}) \n",
    "                counter += 1\n",
    "\n",
    "        sentiments_pd = pd.DataFrame.from_dict(sentiments)\n",
    "\n",
    "        x_vals = sentiments_pd.loc[sentiments_pd[\"User\"] == target][\"Tweets Ago\"]\n",
    "        y_vals = sentiments_pd.loc[sentiments_pd[\"User\"] == target][\"Compound\"]\n",
    "        plt.plot(x_vals,y_vals, marker=\"o\", linewidth=0.5,alpha=0.8)\n",
    "        now = datetime.now()\n",
    "        now = now.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        plt.title(f\"Sentiment Analysis of Tweets ({now}) for {target}\")\n",
    "        plt.xlim([x_vals.max(),x_vals.min()])\n",
    "        plt.ylabel(\"Tweet Polarity\")\n",
    "        plt.xlabel(\"Tweets Ago\")\n",
    "        plt.savefig(f\"Sentiment Analysis for {target}.png\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    for i in range(len(request_user_list)):\n",
    "        api.update_with_media(f'/Users/jingxu/Desktop/Homework_Jing/Homework-JingXu/07SocialAnalytics/PlotBot/Sentiment Analysis for {search_target_list[i]}.png',\n",
    "                          f\"New Tweet Analysis: @{search_target_list[i]} (Thx @{request_user_list[i]})\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry @JingXu35625367 Analyze @INCIndia is not found\n",
      "sorry @JingXu35625367 Analyze @UN is not found\n",
      "sorry @JingXu35625367 @gdebenedetti is not found\n",
      "5\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    SentimentTweet()\n",
    "    time.sleep(30*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
